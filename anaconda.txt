"""
Multipath Scheduler Implementation with Network Testing Framework
This file combines schedulers, network condition simulation, and testing frameworks.
"""

import os
import time
import random
import numpy as np
import threading
from abc import ABC, abstractmethod
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple, Set
from collections import deque, defaultdict
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from enum import Enum
import json
import itertools
import pandas as pd
import xlsxwriter

###########################################
# Part 1: Core Data Structures and Base Classes
###########################################

@dataclass
class Packet:
    id: int
    size: int
    stream_id: str
    deadline: float
    priority: int
    creation_time: float

@dataclass
class PathState:
    id: str
    rtt: float
    bandwidth: float
    cwnd: int
    congestion_level: float
    available_buffer: int
    in_recovery: bool
    last_sent_time: float

@dataclass
class PerformanceMetrics:
    timestamp: float
    latency: float
    jitter: float
    throughput: float
    packet_loss: float
    buffer_occupancy: float
    path_utilization: float
    cwnd_size: int
    queue_length: int
    retransmissions: int

class NetworkCondition(Enum):
    IDEAL = "ideal"
    HIGH_LATENCY = "high_latency"
    PACKET_LOSS = "packet_loss"
    BANDWIDTH_FLUCTUATION = "bandwidth_fluctuation"
    CONGESTION = "congestion"
    ASYMMETRIC = "asymmetric"
    INTERMITTENT = "intermittent"
    MOBILE = "mobile"

class MultipathScenario(Enum):
    DISJOINT_PATHS = "disjoint_paths"
    SHARED_BOTTLENECK = "shared_bottleneck"
    PATH_DIVERSITY = "path_diversity"
    DYNAMIC_PATH = "dynamic_path"
    HETEROGENEOUS_PATHS = "heterogeneous_paths"
    OVERLAPPING_PATHS = "overlapping_paths"
    BACKUP_PATH = "backup_path"
    LOAD_BALANCING = "load_balancing"
    
class TestConfiguration:
    def __init__(self):
        self.topology_types = ['fat-tree', 'jellyfish']
        self.network_scenarios = {
            NetworkCondition.IDEAL: {'name': 'Ideal', 'weight': 1.0},
            NetworkCondition.HIGH_LATENCY: {'name': 'High Latency', 'weight': 1.2},
            NetworkCondition.PACKET_LOSS: {'name': 'Packet Loss', 'weight': 1.3},
            NetworkCondition.BANDWIDTH_FLUCTUATION: {'name': 'Bandwidth Fluctuation', 'weight': 1.1},
            NetworkCondition.CONGESTION: {'name': 'Congestion', 'weight': 1.4},
            NetworkCondition.ASYMMETRIC: {'name': 'Asymmetric', 'weight': 1.2},
            NetworkCondition.INTERMITTENT: {'name': 'Intermittent', 'weight': 1.3},
            NetworkCondition.MOBILE: {'name': 'Mobile', 'weight': 1.5}
        }
        self.multipath_scenarios = {
            MultipathScenario.DISJOINT_PATHS: {'name': 'Disjoint Paths', 'weight': 1.0},
            MultipathScenario.SHARED_BOTTLENECK: {'name': 'Shared Bottleneck', 'weight': 1.3},
            MultipathScenario.PATH_DIVERSITY: {'name': 'Path Diversity', 'weight': 1.2},
            MultipathScenario.DYNAMIC_PATH: {'name': 'Dynamic Path', 'weight': 1.4},
            MultipathScenario.HETEROGENEOUS_PATHS: {'name': 'Heterogeneous Paths', 'weight': 1.2},
            MultipathScenario.OVERLAPPING_PATHS: {'name': 'Overlapping Paths', 'weight': 1.1},
            MultipathScenario.BACKUP_PATH: {'name': 'Backup Path', 'weight': 1.0},
            MultipathScenario.LOAD_BALANCING: {'name': 'Load Balancing', 'weight': 1.2}
        }

###########################################
# Part 2: Virtual Network Simulation
###########################################

@dataclass
class VirtualLink:
    source: str
    target: str
    latency: float
    bandwidth: float
    packet_loss: float
    buffer_size: int = 64000
    current_load: float = 0.0

@dataclass
class VirtualNode:
    id: str
    type: str  # 'host', 'core', 'aggregation', or 'edge'
    connected_links: List[VirtualLink] = None

    def __post_init__(self):
        if self.connected_links is None:
            self.connected_links = []

class VirtualTopology:
    def __init__(self):
        self.nodes: Dict[str, VirtualNode] = {}
        self.links: List[VirtualLink] = []
        self.graph = nx.Graph()
        
    def add_node(self, node_id: str, node_type: str):
        self.nodes[node_id] = VirtualNode(id=node_id, type=node_type)
        self.graph.add_node(node_id, type=node_type)
        
    def add_link(self, source: str, target: str, latency: float, 
                bandwidth: float, packet_loss: float):
        link = VirtualLink(source=source, target=target, 
                         latency=latency, bandwidth=bandwidth,
                         packet_loss=packet_loss)
        self.links.append(link)
        self.nodes[source].connected_links.append(link)
        self.nodes[target].connected_links.append(link)
        self.graph.add_edge(source, target, link=link)

class FatTreeTopology(VirtualTopology):
    def __init__(self, k: int = 8):
        """Initialize a k-ary fat-tree topology"""
        super().__init__()
        self.k = k
        self.pod = k
        self.core_switches = (k//2)**2
        self.aggr_switches_per_pod = k//2
        self.edge_switches_per_pod = k//2
        self.hosts_per_edge = k//2
        
        self._create_topology()
        
    def _create_topology(self):
        """Create the fat-tree topology"""
        # Create core switches
        for i in range(self.core_switches):
            self.add_node(f'c{i}', 'core')
            
        # Create aggregation and edge switches, and hosts
        for pod in range(self.pod):
            # Add aggregation switches
            for switch in range(self.aggr_switches_per_pod):
                self.add_node(f'a{pod}{switch}', 'aggregation')
                
            # Add edge switches
            for switch in range(self.edge_switches_per_pod):
                self.add_node(f'e{pod}{switch}', 'edge')
                
                # Add hosts for each edge switch
                for host in range(self.hosts_per_edge):
                    self.add_node(f'h{pod}{switch}{host}', 'host')
        
        self._create_links()
        
    def _create_links(self):
        """Create links between nodes"""
        # Connect core to aggregation
        core_index = 0
        for pod in range(self.pod):
            for aggr in range(self.aggr_switches_per_pod):
                for j in range(self.k//2):
                    self.add_link(
                        f'c{core_index}', 
                        f'a{pod}{aggr}',
                        latency=random.uniform(1, 5),
                        bandwidth=random.uniform(80, 100),
                        packet_loss=random.uniform(0, 0.001)
                    )
                    core_index = (core_index + 1) % self.core_switches
                    
        # Connect aggregation to edge
        for pod in range(self.pod):
            for aggr in range(self.aggr_switches_per_pod):
                for edge in range(self.edge_switches_per_pod):
                    self.add_link(
                        f'a{pod}{aggr}', 
                        f'e{pod}{edge}',
                        latency=random.uniform(1, 5),
                        bandwidth=random.uniform(80, 100),
                        packet_loss=random.uniform(0, 0.001)
                    )
                    
        # Connect edge to hosts
        for pod in range(self.pod):
            for edge in range(self.edge_switches_per_pod):
                for host in range(self.hosts_per_edge):
                    self.add_link(
                        f'e{pod}{edge}', 
                        f'h{pod}{edge}{host}',
                        latency=random.uniform(0.1, 1),
                        bandwidth=random.uniform(90, 100),
                        packet_loss=random.uniform(0, 0.0001)
                    )

class JellyfishTopology(VirtualTopology):
    def __init__(self, n_switches: int = 64, n_ports: int = 8, n_hosts: int = 64):
        """Initialize a Jellyfish topology"""
        super().__init__()
        self.n_switches = n_switches
        self.n_ports = n_ports
        self.n_hosts = n_hosts
        
        self._create_topology()
        
    def _create_topology(self):
        """Create the Jellyfish topology"""
        # Create switches
        for i in range(self.n_switches):
            self.add_node(f's{i}', 'switch')
            
        # Create hosts and connect them to switches
        for i in range(self.n_hosts):
            self.add_node(f'h{i}', 'host')
            self.add_link(
                f'h{i}', 
                f's{i % self.n_switches}',
                latency=random.uniform(0.1, 1),
                bandwidth=random.uniform(90, 100),
                packet_loss=random.uniform(0, 0.0001)
            )
            
        # Create random switch-to-switch connections
        G = nx.random_regular_graph(self.n_ports - 1, self.n_switches)
        for (u, v) in G.edges():
            self.add_link(
                f's{u}', 
                f's{v}',
                latency=random.uniform(1, 5),
                bandwidth=random.uniform(80, 100),
                packet_loss=random.uniform(0, 0.001)
            )

class NetworkSimulator:
    def __init__(self, topology: VirtualTopology):
        self.topology = topology
        self.active_flows = defaultdict(list)
        self.path_cache = {}
        
    def get_paths(self, source: str, destination: str, k: int = 4) -> List[List[str]]:
        """Get k-shortest paths between source and destination"""
        cache_key = (source, destination, k)
        if cache_key in self.path_cache:
            return self.path_cache[cache_key]
            
        try:
            paths = list(itertools.islice(
                nx.shortest_simple_paths(self.topology.graph, source, destination), 
                k
            ))
            self.path_cache[cache_key] = paths
            return paths
        except:
            return []
            
    def simulate_flow(self, source: str, destination: str, flow_size: int) -> Dict:
        """Simulate a flow between two nodes and return performance metrics"""
        paths = self.get_paths(source, destination)
        if not paths:
            return None
            
        # Select least congested path
        selected_path = min(paths, key=lambda p: self._calculate_path_congestion(p))
        
        # Calculate metrics
        latency = self._calculate_path_latency(selected_path)
        bandwidth = self._calculate_path_bandwidth(selected_path)
        packet_loss = self._calculate_path_packet_loss(selected_path)
        
        # Update link loads
        self._update_link_loads(selected_path, flow_size)
        
        return {
            'path': selected_path,
            'latency': latency,
            'bandwidth': bandwidth,
            'packet_loss': packet_loss
        }
        
    def _calculate_path_congestion(self, path: List[str]) -> float:
        """Calculate congestion level of a path"""
        congestion = 0
        for i in range(len(path) - 1):
            link = self._get_link(path[i], path[i+1])
            if link:
                congestion += link.current_load / link.bandwidth
        return congestion / (len(path) - 1) if len(path) > 1 else float('inf')
        
    def _calculate_path_latency(self, path: List[str]) -> float:
        """Calculate total latency along a path"""
        latency = 0
        for i in range(len(path) - 1):
            link = self._get_link(path[i], path[i+1])
            if link:
                latency += link.latency * (1 + link.current_load / link.bandwidth)
        return latency
        
    def _calculate_path_bandwidth(self, path: List[str]) -> float:
        """Calculate available bandwidth along a path"""
        return min(
            self._get_link(path[i], path[i+1]).bandwidth - 
            self._get_link(path[i], path[i+1]).current_load
            for i in range(len(path) - 1)
        )
        
    def _calculate_path_packet_loss(self, path: List[str]) -> float:
        """Calculate cumulative packet loss probability along a path"""
        loss_prob = 1.0
        for i in range(len(path) - 1):
            link = self._get_link(path[i], path[i+1])
            if link:
                loss_prob *= (1 - link.packet_loss)
        return 1 - loss_prob
        
    def _get_link(self, source: str, target: str) -> Optional[VirtualLink]:
        """Get link between two nodes"""
        for link in self.topology.links:
            if (link.source == source and link.target == target) or \
               (link.source == target and link.target == source):
                return link
        return None
        
    def _update_link_loads(self, path: List[str], flow_size: int):
        """Update current load on links along a path"""
        for i in range(len(path) - 1):
            link = self._get_link(path[i], path[i+1])
            if link:
                link.current_load += flow_size

###########################################
# Part 3: Scheduler Base Class
###########################################

class BaseScheduler(ABC):
    def __init__(self):
        self.paths: Dict[str, PathState] = {}
        self.packets_queue: deque = deque()
        self.sent_packets: Dict[int, float] = {}
        self.stats = {
            'packets_sent': 0,
            'bytes_sent': 0,
            'retransmissions': 0
        }
        self.running = True
        
    @abstractmethod
    def select_path(self, packet: Packet) -> Optional[PathState]:
        pass
    
    def update_path_state(self, path_id: str, **kwargs):
        if path_id in self.paths:
            for key, value in kwargs.items():
                setattr(self.paths[path_id], key, value)
                
    def add_packet(self, packet: Packet):
        self.packets_queue.append(packet)
        
    def get_path_stats(self, path_id: str) -> dict:
        if path_id not in self.paths:
            return {}
        path = self.paths[path_id]
        return {
            'rtt': path.rtt,
            'bandwidth': path.bandwidth,
            'cwnd': path.cwnd,
            'congestion_level': path.congestion_level
        }

    def run_scheduler_loop(self):
        while self.running:
            if self.packets_queue:
                packet = self.packets_queue[0]
                
                selected_path = self.select_path(packet)
                if selected_path:
                    self.packets_queue.popleft()
                    self.sent_packets[packet.id] = time.time()
                    self.stats['packets_sent'] += 1
                    self.stats['bytes_sent'] += packet.size
                    
                    selected_path.available_buffer -= packet.size
                    selected_path.last_sent_time = time.time()
                    
            time.sleep(0.001)

###########################################
# Part 4: Test Environment Setup
###########################################

def create_test_environment():
    """Create a test environment with four diverse paths"""
    # Path 1: Low latency, high bandwidth
    path1 = PathState(
        id="path1",
        rtt=25.0,  # 25ms - very low latency
        bandwidth=200.0,  # 200 Mbps - high bandwidth
        cwnd=1000,
        congestion_level=0.1,
        available_buffer=64000,
        in_recovery=False,
        last_sent_time=0.0
    )
    
    # Path 2: Medium latency, medium bandwidth
    path2 = PathState(
        id="path2",
        rtt=50.0,  # 50ms - medium latency
        bandwidth=150.0,  # 150 Mbps - medium-high bandwidth
        cwnd=1000,
        congestion_level=0.2,
        available_buffer=64000,
        in_recovery=False,
        last_sent_time=0.0
    )
    
    # Path 3: High latency, high bandwidth
    path3 = PathState(
        id="path3",
        rtt=100.0,  # 100ms - high latency
        bandwidth=300.0,  # 300 Mbps - very high bandwidth
        cwnd=1000,
        congestion_level=0.15,
        available_buffer=64000,
        in_recovery=False,
        last_sent_time=0.0
    )
    
    # Path 4: Low latency, low bandwidth
    path4 = PathState(
        id="path4",
        rtt=30.0,  # 30ms - low latency
        bandwidth=50.0,  # 50 Mbps - low bandwidth
        cwnd=1000,
        congestion_level=0.05,
        available_buffer=64000,
        in_recovery=False,
        last_sent_time=0.0
    )
    
    return {
        "path1": path1,
        "path2": path2,
        "path3": path3,
        "path4": path4
    }

def create_monitoring_service():
    """Create a monitoring service for scheduler comparison"""
    return {
        'ECF': SchedulerMonitor('ECF'),
        'RR': SchedulerMonitor('RR'),
        'MPQUIC': SchedulerMonitor('MPQUIC'),
        'Peekabo': SchedulerMonitor('Peekabo'),
        "BLEST": SchedulerMonitor('BLEST'),
        "MAB": SchedulerMonitor('MAB'),
        "MAMS": SchedulerMonitor('MAMS'),
    }

###########################################
# Part 5: Scheduler Implementations
###########################################

class ECFScheduler(BaseScheduler):
    """Earliest Completion First Scheduler Implementation"""
 
    def __init__(self):
        super().__init__()
        self.path_history = defaultdict(list)
        self.smoothing_factor = 0.125
        self.min_rtt_samples = 5
        self.congestion_threshold = 0.8
        
    def select_path(self, packet: Packet) -> Optional[PathState]:
        """Select path using ECF algorithm with enhanced decision making"""
        eligible_paths = self._get_eligible_paths(packet)
        if not eligible_paths:
            return None
            
        path_scores = []
        current_time = time.time()
        
        for path in eligible_paths:
            transmission_time = packet.size / path.bandwidth
            propagation_time = path.rtt / 2
            queuing_delay = self._estimate_queuing_delay(path)
            
            completion_time = transmission_time + propagation_time + queuing_delay
            completion_time = self._apply_penalties(
                completion_time, path, packet, current_time
            )
            
            path_scores.append((completion_time, path))
        
        if path_scores:
            best_path = min(path_scores, key=lambda x: x[0])[1]
            self._update_path_history(best_path.id)
            return best_path
            
        return None      
      
    def _get_eligible_paths(self, packet: Packet) -> List[PathState]:
        """Filter paths based on eligibility criteria"""
        eligible_paths = []
        
        for path in self.paths.values():
            # Check basic eligibility
            if (path.available_buffer >= packet.size and
                not self._is_path_congested(path)):
                
                # Check if path meets deadline requirements
                estimated_delivery = (time.time() + path.rtt + 
                                   packet.size / path.bandwidth)
                if packet.deadline == 0 or estimated_delivery <= packet.deadline:
                    eligible_paths.append(path)
                    
        return eligible_paths
        
    def _estimate_queuing_delay(self, path: PathState) -> float:
        """Estimate queuing delay based on buffer occupancy"""
        buffer_occupancy = 1 - (path.available_buffer / 64000)  # Assuming 64KB buffer
        return buffer_occupancy * path.rtt
        
    def _is_path_congested(self, path: PathState) -> bool:
        """Determine if path is congested based on multiple factors"""
        if path.congestion_level > self.congestion_threshold:
            return True
            
        # Check recent RTT trend
        rtt_samples = self.path_history[path.id][-self.min_rtt_samples:]
        if len(rtt_samples) >= self.min_rtt_samples:
            rtt_trend = np.mean(np.diff(rtt_samples))
            if rtt_trend > 0.1 * path.rtt:  # RTT increasing significantly
                return True
                
        return False
        
    def _apply_penalties(self, base_time: float, path: PathState, 
                        packet: Packet, current_time: float) -> float:
        """Apply penalties to completion time based on various factors"""
        completion_time = base_time
        
        # Penalty for congestion level
        completion_time *= (1 + path.congestion_level)
        
        # Penalty for recent failures or retransmissions
        if path.in_recovery:
            completion_time *= 1.5
            
        # Penalty for path instability
        stability_factor = self._calculate_path_stability(path.id)
        completion_time *= (1 + (1 - stability_factor))
        
        # Priority-based adjustment
        if packet.priority > 0:
            completion_time /= packet.priority
            
        return completion_time
        
    def _calculate_path_stability(self, path_id: str) -> float:
        """Calculate path stability based on historical performance"""
        history = self.path_history[path_id]
        if len(history) < 2:
            return 1.0
            
        # Calculate RTT variance
        rtt_variance = np.var(history) / np.mean(history)
        
        # Convert to stability score (0 to 1)
        stability = 1 / (1 + rtt_variance)
        return stability
        
    def _update_path_history(self, path_id: str):
        """Update path history with latest RTT measurement"""
        current_rtt = self.paths[path_id].rtt
        history = self.path_history[path_id]
        
        # Apply EWMA for smoothing
        if history:
            smoothed_rtt = (self.smoothing_factor * current_rtt + 
                          (1 - self.smoothing_factor) * history[-1])
            history.append(smoothed_rtt)
        else:
            history.append(current_rtt)
            
        # Keep history bounded
        if len(history) > 100:
            history.pop(0)

class RRScheduler(BaseScheduler):
    """Round Robin Scheduler Implementation with enhancements"""
    
    def __init__(self):
        super().__init__()
        self.current_path_index = 0
        self.path_weights = {}
        self.path_counters = defaultdict(int)
        self.last_adjustment_time = time.time()
        self.adjustment_interval = 1.0
        
    def select_path(self, packet: Packet) -> Optional[PathState]:
        """Select path using weighted round-robin with dynamic adjustment"""
        eligible_paths = self._get_eligible_paths(packet)
        if not eligible_paths:
            return None
            
        current_time = time.time()
        if current_time - self.last_adjustment_time > self.adjustment_interval:
            self._adjust_weights()
            self.last_adjustment_time = current_time
            
        selected_path = self._weighted_round_robin_select(eligible_paths)
        if selected_path:
            self.path_counters[selected_path.id] += 1
            
        return selected_path
        
    def _get_eligible_paths(self, packet: Packet) -> List[PathState]:
        """Filter paths based on eligibility criteria"""
        return [path for path in self.paths.values()
                if path.available_buffer >= packet.size
                and not path.in_recovery]
                
    def _adjust_weights(self):
        """Dynamically adjust path weights based on performance"""
        total_bandwidth = sum(path.bandwidth for path in self.paths.values())
        total_packets = sum(self.path_counters.values()) or 1
        
        for path_id, path in self.paths.items():
            # Calculate base weight from bandwidth proportion
            bandwidth_weight = path.bandwidth / total_bandwidth
            
            # Adjust weight based on RTT
            rtt_factor = min(1.0, 50.0 / path.rtt)  # Normalize RTT impact
            
            # Adjust weight based on congestion
            congestion_factor = 1.0 - path.congestion_level
            
            # Adjust weight based on historical usage
            usage_rate = self.path_counters[path_id] / total_packets
            balance_factor = 1.0 - usage_rate
            
            # Combine factors
            self.path_weights[path_id] = (bandwidth_weight * 
                                        rtt_factor * 
                                        congestion_factor * 
                                        balance_factor)
            
        # Normalize weights
        weight_sum = sum(self.path_weights.values())
        if weight_sum > 0:
            for path_id in self.path_weights:
                self.path_weights[path_id] /= weight_sum
                
    def _weighted_round_robin_select(self, 
                                   eligible_paths: List[PathState]) -> Optional[PathState]:
        """Select path using weighted round-robin algorithm"""
        if not eligible_paths:
            return None
            
        # Filter weights for eligible paths
        eligible_weights = {path.id: self.path_weights.get(path.id, 1.0)
                          for path in eligible_paths}
        
        # Normalize weights for eligible paths
        weight_sum = sum(eligible_weights.values())
        if weight_sum > 0:
            normalized_weights = {
                path_id: weight / weight_sum
                for path_id, weight in eligible_weights.items()
            }
        else:
            # Equal weights if all weights are zero
            weight = 1.0 / len(eligible_paths)
            normalized_weights = {path.id: weight for path in eligible_paths}
            
        # Select path based on weights
        selection = random.random()
        cumulative = 0.0
        
        for path in eligible_paths:
            cumulative += normalized_weights[path.id]
            if selection <= cumulative:
                return path
                
        return eligible_paths[-1]  # Fallback to last path if no selection made

class MPQUICScheduler(BaseScheduler):
    
    def __init__(self):
        self.paths = []
        self.streams = []
        self.metrics = {
            'path_history': {},
            'stream_states': {},
            'prediction_models': {}
        }
        self.parameters = {
            'rtt_threshold': 0.1,  # 100ms
            'cwnd_min': 10,
            'buffer_size': 1000,
            'priority_levels': 5,
            'redundancy_threshold': 0.8
        }
        
    def calculate_smoothed_rtt(self, path: PathState) -> float:
        if not path.rtt_history:
            return 0
        alpha = 0.125
        smoothed = 0
        for rtt in path.rtt_history:
            smoothed = alpha * rtt + (1 - alpha) * smoothed
        return smoothed

    def calculate_variance(self, values: deque) -> float:
        return np.var(list(values)) if values else 0

    def measure_throughput(self, path: PathState) -> float:
        if not path.bandwidth_history:
            return 0
        return np.mean(list(path.bandwidth_history))

    def calculate_loss_rate(self, path: PathState) -> float:
        if not path.loss_history:
            return 0
        return sum(path.loss_history) / len(path.loss_history)

    def collect_metrics(self) -> tuple:
        path_metrics = {}
        stream_metrics = {}

        # Collect path metrics
        for path in self.paths:
            rtt_metrics = {
                'smoothed_rtt': self.calculate_smoothed_rtt(path),
                'rtt_variance': self.calculate_variance(path.rtt_history),
                'min_rtt': min(path.rtt_history) if path.rtt_history else 0,
                'rtt_trend': self.analyze_trend(path.rtt_history)
            }

            bandwidth_metrics = {
                'current_throughput': self.measure_throughput(path),
                'available_bandwidth': self.estimate_bandwidth(path),
                'stability': self.calculate_stability(path),
                'trend': self.analyze_trend(path.bandwidth_history)
            }

            path_metrics[path.id] = {
                'rtt': rtt_metrics,
                'bandwidth': bandwidth_metrics,
                'loss': {'loss_rate': self.calculate_loss_rate(path)},
                'timestamp': time.time()
            }

        # Collect stream metrics
        for stream in self.streams:
            stream_metrics[stream.id] = {
                'buffer': {
                    'occupancy': stream.buffer.get('occupancy', 0),
                    'drain_rate': stream.buffer.get('drain_rate', 0)
                },
                'deadline': stream.deadline,
                'priority': stream.priority,
                'timestamp': time.time()
            }

        return path_metrics, stream_metrics

    def calculate_scores(self, path_metrics: Dict, stream_metrics: Dict) -> Dict:
        scores = {'paths': {}, 'streams': {}}

        # Calculate path scores
        for path_id, metrics in path_metrics.items():
            performance = 1 / (metrics['rtt']['smoothed_rtt'] + 1e-6)
            reliability = 1 - metrics['loss']['loss_rate']
            bandwidth = metrics['bandwidth']['current_throughput']
            
            # Combine scores with weights
            scores['paths'][path_id] = (
                0.4 * performance +
                0.3 * reliability +
                0.3 * bandwidth
            )

        # Calculate stream scores
        for stream_id, metrics in stream_metrics.items():
            urgency = 1 / (metrics['deadline'] + 1e-6)
            priority = metrics['priority'] / self.parameters['priority_levels']
            
            scores['streams'][stream_id] = (
                0.6 * urgency +
                0.4 * priority
            )

        return scores

    def prioritize_streams(self, streams: List[Packet], scores: Dict) -> Dict:
        prioritized = {
            'urgent': [],
            'normal': [],
            'background': []
        }

        for stream in streams:
            score = scores['streams'][stream.id]
            if score > 0.8:
                prioritized['urgent'].append(stream)
            elif score > 0.4:
                prioritized['normal'].append(stream)
            else:
                prioritized['background'].append(stream)

        # Sort each category by scores
        for category in prioritized:
            prioritized[category].sort(
                key=lambda x: scores['streams'][x.id],
                reverse=True
            )

        return prioritized
        
    def select_path(self, packet: Packet, paths: Optional[Dict[str, PathState]] = None, scores: Optional[Dict[str, float]] = None) -> Optional[PathState]:
        assignments = {}
        available_paths = [p for p in paths if scores['paths'][p.id] > 0.3]

        for category in ['urgent', 'normal', 'background']:
            for stream in prioritized_streams[category]:
                if not available_paths:
                    continue

                # Find best path based on scores
                best_path = max(available_paths,
                              key=lambda p: scores['paths'][p.id])
                assignments[stream.id] = best_path

                # Handle redundancy for urgent streams
                if (category == 'urgent' and 
                    scores['streams'][stream.id] > self.parameters['redundancy_threshold']):
                    remaining_paths = [p for p in available_paths if p != best_path]
                    if remaining_paths:
                        backup_path = max(remaining_paths,
                                        key=lambda p: scores['paths'][p.id])
                        assignments[f"{stream.id}_backup"] = backup_path

        return assignments

    def run_scheduler_loop(self):
        while True:
            cycle_start = time.time()

            # Collect metrics
            path_metrics, stream_metrics = self.collect_metrics()

            # Calculate scores
            scores = self.calculate_scores(path_metrics, stream_metrics)

            # Prioritize streams
            prioritized = self.prioritize_streams(self.streams, scores)

            # Select paths
            assignments = self.select_paths(prioritized, self.paths, scores)

            # Apply assignments (implementation depends on specific QUIC stack)
            self.apply_assignments(assignments)

            # Update statistics
            self.update_statistics(path_metrics, stream_metrics)

            # Calculate wait time
            cycle_duration = time.time() - cycle_start
            wait_time = max(0.01, 0.05 - cycle_duration)  # 50ms target interval
            time.sleep(wait_time)

    def estimate_bandwidth(self, path: PathState) -> float:
        return np.mean(list(path.bandwidth_history)) if path.bandwidth_history else 0

    def calculate_stability(self, path: PathState) -> float:
        return 1 - self.calculate_variance(path.bandwidth_history)

    def analyze_trend(self, values: deque) -> float:
        if len(values) < 2:
            return 0
        values_list = list(values)
        return np.polyfit(range(len(values_list)), values_list, 1)[0]

    def apply_assignments(self, assignments: Dict):
        # Implementation depends on specific QUIC stack
        pass

    def update_statistics(self, path_metrics: Dict, stream_metrics: Dict):
        # Update historical metrics
        for path_id, metrics in path_metrics.items():
            self.metrics['path_history'].setdefault(path_id, []).append(metrics)

        for stream_id, metrics in stream_metrics.items():
            self.metrics['stream_states'].setdefault(stream_id, []).append(metrics)
            
class PeekaboScheduler(BaseScheduler):
    """Peekabo Scheduler Implementation"""
    
    def select_path(self, packet: Packet) -> Optional[PathState]:
        # Peekabo logic: prioritize paths with the lowest latency and minimal congestion.
        eligible_paths = [
            path for path in self.paths.values()
            if path.available_buffer >= packet.size and not path.in_recovery
        ]
        
        if not eligible_paths:
            return None
        
        # Score paths based on latency and congestion level
        scored_paths = [
            (path.rtt * (1 + path.congestion_level), path)
            for path in eligible_paths
        ]
        
        # Choose the path with the lowest score
        return min(scored_paths, key=lambda x: x[0])[1]


class BLESTScheduler(BaseScheduler):
    """BLEST Scheduler Implementation"""
    
    def select_path(self, packet: Packet) -> Optional[PathState]:
        # BLEST logic: predict delays and avoid paths likely to cause retransmissions.
        eligible_paths = [
            path for path in self.paths.values()
            if path.available_buffer >= packet.size and not path.in_recovery
        ]
        
        if not eligible_paths:
            return None
        
        scored_paths = []
        for path in eligible_paths:
            predicted_delay = path.rtt + (packet.size / path.bandwidth)
            penalty = path.congestion_level * predicted_delay
            scored_paths.append((predicted_delay + penalty, path))
        
        # Select the path with the lowest predicted delay
        return min(scored_paths, key=lambda x: x[0])[1]


class MABScheduler(BaseScheduler):
    """Multi-Armed Bandit Scheduler Implementation"""
    
    def __init__(self):
        super().__init__()
        self.path_rewards = defaultdict(float)
        self.path_attempts = defaultdict(int)

    def select_path(self, packet: Packet) -> Optional[PathState]:
        eligible_paths = [
            path for path in self.paths.values()
            if path.available_buffer >= packet.size and not path.in_recovery
        ]

        if not eligible_paths:
            return None

        # Exploration vs. Exploitation
        exploration_rate = 0.1  # 10% exploration
        if random.random() < exploration_rate:
            return random.choice(eligible_paths)

        # Exploitation: Choose the path with the highest reward-to-attempt ratio
        scored_paths = [
            (self.path_rewards[path.id] / (self.path_attempts[path.id] + 1), path)
            for path in eligible_paths
        ]
        selected_path = max(scored_paths, key=lambda x: x[0])[1]

        # Update attempts
        self.path_attempts[selected_path.id] += 1
        return selected_path

    def update_path_state(self, path_id: str, **kwargs):
        super().update_path_state(path_id, **kwargs)

        # Update rewards based on RTT or other performance metrics
        path = self.paths[path_id]
        reward = 1 / (path.rtt + 1e-6)  # Higher reward for lower RTT
        self.path_rewards[path_id] += reward


class MAMSScheduler(BaseScheduler):
    """MAMS Scheduler Implementation"""
    
    def select_path(self, packet: Packet) -> Optional[PathState]:
        eligible_paths = [
            path for path in self.paths.values()
            if path.available_buffer >= packet.size and not path.in_recovery
        ]

        if not eligible_paths:
            return None

        # Adaptively prioritize paths based on latency, bandwidth, and congestion
        scored_paths = []
        for path in eligible_paths:
            score = (
                0.5 * (1 / (path.rtt + 1e-6)) +  # Lower latency preferred
                0.3 * path.bandwidth +           # Higher bandwidth preferred
                0.2 * (1 - path.congestion_level) # Lower congestion preferred
            )
            scored_paths.append((score, path))

        # Select the path with the highest score
        return max(scored_paths, key=lambda x: x[0])[1]

###########################################
# Part 6: Monitoring and Analysis Systems
###########################################

class SchedulerMonitor:
    """Monitor and collect metrics for scheduler performance"""
    
    def __init__(self, scheduler_name: str):
        self.scheduler_name = scheduler_name
        self.metrics_history = []
        self.start_time = None
        self.is_monitoring = False
        self.sampling_interval = 0.1  # 100ms sampling
        self.monitor_thread = None
        self.analysis_results = {}

    def start_monitoring(self):
        self.start_time = time.time()
        self.is_monitoring = True
        self.metrics_history.clear()
        self.monitor_thread = threading.Thread(target=self._monitoring_loop)
        self.monitor_thread.daemon = True
        self.monitor_thread.start()

    def stop_monitoring(self):
        self.is_monitoring = False
        if self.monitor_thread:
            self.monitor_thread.join()
        self._analyze_results()

    def _monitoring_loop(self):
        while self.is_monitoring:
            metrics = self._collect_metrics()
            self.metrics_history.append(metrics)
            time.sleep(self.sampling_interval)

    def _collect_metrics(self) -> PerformanceMetrics:
        current_time = time.time()
        latency = random.uniform(10, 100)  # Simulate latency in ms
        jitter = random.uniform(1, 10)  # Simulate jitter in ms
        throughput = random.uniform(50, 100)  # Simulate throughput in Mbps
        packet_loss = random.uniform(0.01, 0.05)  # Simulate packet loss rate
        buffer_occupancy = random.uniform(0, 1) * 64000  # Simulate buffer usage
        path_utilization = random.uniform(0.5, 1.0)  # Simulate path utilization
        cwnd_size = random.randint(500, 1000)  # Simulate congestion window size
        queue_length = random.randint(0, 50)  # Simulate queue length
        retransmissions = random.randint(0, 10)  # Simulate retransmissions

        return PerformanceMetrics(
            timestamp=current_time - self.start_time,
            latency=latency,
            jitter=jitter,
            throughput=throughput,
            packet_loss=packet_loss,
            buffer_occupancy=buffer_occupancy,
            path_utilization=path_utilization,
            cwnd_size=cwnd_size,
            queue_length=queue_length,
            retransmissions=retransmissions,
        )

    def _analyze_results(self):
        if not self.metrics_history:
            return

        metrics_array = np.array([
            [m.latency, m.throughput, m.packet_loss, m.path_utilization]
            for m in self.metrics_history
        ])

        self.analysis_results = {
            'latency': {
                'mean': np.mean(metrics_array[:, 0]),
                'std': np.std(metrics_array[:, 0]),
                'percentile_95': np.percentile(metrics_array[:, 0], 95)
            },
            'throughput': {
                'mean': np.mean(metrics_array[:, 1]),
                'std': np.std(metrics_array[:, 1]),
                'stability': 1.0 / (1.0 + np.std(metrics_array[:, 1]) / np.mean(metrics_array[:, 1]))
            },
            'packet_loss': {
                'mean': np.mean(metrics_array[:, 2]),
                'max': np.max(metrics_array[:, 2])
            },
            'path_utilization': {
                'mean': np.mean(metrics_array[:, 3]),
                'efficiency': np.mean(metrics_array[:, 3]) * (
                    1.0 - np.std(metrics_array[:, 3]) / np.max(metrics_array[:, 3])
                )
            },
            'overall_score': self._calculate_overall_score(metrics_array)
        }

    def _calculate_stability(self, metric_series: np.ndarray) -> float:
        """Calculate stability score based on metric variance"""
        if len(metric_series) < 2:
            return 1.0
        return 1.0 / (1.0 + np.std(metric_series) / np.mean(metric_series))
        
    def _calculate_efficiency(self, utilization_series: np.ndarray) -> float:
        """Calculate path utilization efficiency"""
        return np.mean(utilization_series) * (
            1.0 - np.std(utilization_series) / np.max(utilization_series)
        )
        
    def _calculate_overall_score(self, metrics_array: np.ndarray) -> float:
        """Calculate overall performance score"""
        weights = {
            'latency': 0.3,
            'throughput': 0.3,
            'packet_loss': 0.2,
            'utilization': 0.2
        }
        
        # Normalize metrics
        normalized_metrics = self._normalize_metrics(metrics_array)
        
        # Calculate weighted score
        score = (
            weights['latency'] * (1 - normalized_metrics[:, 0].mean()) +  # Lower latency is better
            weights['throughput'] * normalized_metrics[:, 1].mean() +     # Higher throughput is better
            weights['packet_loss'] * (1 - normalized_metrics[:, 2].mean()) +  # Lower loss is better
            weights['utilization'] * normalized_metrics[:, 3].mean()      # Higher utilization is better
        )
        
        return score
        
    def _normalize_metrics(self, metrics_array: np.ndarray) -> np.ndarray:
        """Normalize metrics to [0, 1] range"""
        normalized = np.zeros_like(metrics_array)
        
        for i in range(metrics_array.shape[1]):
            min_val = np.min(metrics_array[:, i])
            max_val = np.max(metrics_array[:, i])
            
            if max_val > min_val:
                normalized[:, i] = (metrics_array[:, i] - min_val) / (max_val - min_val)
            else:
                normalized[:, i] = 0.5  # Default value if no variation
                
        return normalized

class PerformanceAnalyzer:
    def __init__(self, monitors: Dict[str, SchedulerMonitor]):
        self.monitors = monitors
        self.comparison_results = {}
        
    def analyze_all(self):
        """Perform comprehensive analysis of all schedulers"""
        # First ensure all monitors have analyzed their results
        for monitor in self.monitors.values():
            if not hasattr(monitor, 'analysis_results'):
                monitor.analysis_results = {}
            if 'overall_score' not in monitor.analysis_results:
                # Calculate basic score if none exists
                monitor.analysis_results['overall_score'] = self._calculate_basic_score(monitor)
        
        self.comparison_results = {
            'overall_ranking': self._rank_schedulers(),
            'detailed_comparison': self._compare_metrics(),
            'stability_analysis': self._analyze_stability(),
            'efficiency_analysis': self._analyze_efficiency(),
            'recommendations': self._generate_recommendations()
        }
        
    def _calculate_basic_score(self, monitor: SchedulerMonitor) -> float:
        """Calculate a basic score based on available metrics"""
        if not monitor.metrics_history:
            return 0.0
        
        # Calculate average metrics
        latency_scores = []
        throughput_scores = []
        for metric in monitor.metrics_history:
            if hasattr(metric, 'latency'):
                latency_scores.append(metric.latency)
            if hasattr(metric, 'throughput'):
                throughput_scores.append(metric.throughput)
            
        # Normalize and combine scores
        score = 0.0
    
        # Handle latency scores
        if latency_scores:
            max_latency = max(latency_scores)
            if max_latency > 0:
                latency_score = 1 - (sum(latency_scores) / len(latency_scores) / max_latency)
                score += latency_score * 0.5
        
        # Handle throughput scores
        if throughput_scores:
            max_throughput = max(throughput_scores)
            if max_throughput > 0:
                throughput_score = sum(throughput_scores) / len(throughput_scores) / max_throughput
                score += throughput_score * 0.5
        
        return max(0.0, min(1.0, score))  # Ensure score is between 0 and 1
        
    def _rank_schedulers(self) -> Dict[str, float]:
        """Rank schedulers based on overall performance"""
        scores = {}
        for name, monitor in self.monitors.items():
            scores[name] = monitor.analysis_results.get('overall_score', 0.0)
        
        # Normalize scores, handling the case where all scores are 0
        max_score = max(scores.values())
        if max_score == 0:
            # If all scores are 0, return equal rankings
            return {name: 1.0 for name in scores.keys()}
    
        return {name: score/max_score for name, score in scores.items()}
        
    def _compare_metrics(self) -> Dict:
        """Compare detailed metrics across schedulers"""
        metrics_comparison = {}
        
        for metric in ['latency', 'throughput', 'packet_loss', 'path_utilization']:
            metric_data = {}
            for name, monitor in self.monitors.items():
                metric_data[name] = monitor.analysis_results[metric]
            metrics_comparison[metric] = metric_data
            
        return metrics_comparison
        
    def _analyze_stability(self) -> Dict:
        """Analyze stability characteristics of each scheduler"""
        stability_analysis = {}
        
        for name, monitor in self.monitors.items():
            metrics = monitor.metrics_history
            stability_analysis[name] = {
                'throughput_stability': self._calculate_stability_score(
                    [m.throughput for m in metrics]
                ),
                'latency_stability': self._calculate_stability_score(
                    [m.latency for m in metrics]
                ),
                'overall_stability': monitor.analysis_results['throughput']['stability']
            }
            
        return stability_analysis
        
    def _analyze_efficiency(self) -> Dict:
        """Analyze resource utilization efficiency"""
        efficiency_analysis = {}
        
        for name, monitor in self.monitors.items():
            metrics = monitor.metrics_history
            efficiency_analysis[name] = {
                'path_efficiency': monitor.analysis_results['path_utilization']['efficiency'],
                'resource_utilization': self._calculate_resource_efficiency(metrics),
                'adaptation_speed': self._calculate_adaptation_speed(metrics)
            }
            
        return efficiency_analysis
        
    def _generate_recommendations(self) -> Dict:
        """Generate recommendations based on analysis"""
        recommendations = {}
        
        # Analyze each scheduler's strengths and weaknesses
        for name, monitor in self.monitors.items():
            analysis = monitor.analysis_results
            strengths = []
            weaknesses = []
            
            # Analyze latency
            if analysis['latency']['mean'] < 50:  # Example threshold
                strengths.append("Low latency")
            else:
                weaknesses.append("High latency")
                
            # Analyze throughput
            if analysis['throughput']['stability'] > 0.8:
                strengths.append("Stable throughput")
            else:
                weaknesses.append("Unstable throughput")
                
            # Generate recommendations
            recommendations[name] = {
                'strengths': strengths,
                'weaknesses': weaknesses,
                'suggested_improvements': self._suggest_improvements(weaknesses),
                'best_use_cases': self._determine_use_cases(strengths)
            }
            
        return recommendations
        
    def _calculate_stability_score(self, metric_series: List[float]) -> float:
        """Calculate stability score for a metric series"""
        if not metric_series:
            return 0.0
            
        values = np.array(metric_series)
        return 1.0 / (1.0 + np.std(values) / np.mean(values))
        
    def _calculate_resource_efficiency(self, metrics: List[PerformanceMetrics]) -> float:
        """Calculate resource utilization efficiency"""
        if not metrics:
            return 0.0
            
        throughput_values = np.array([m.throughput for m in metrics])
        buffer_usage = np.array([m.buffer_occupancy for m in metrics])
        
        # Combine throughput efficiency with buffer efficiency
        throughput_efficiency = np.mean(throughput_values) / np.max(throughput_values)
        buffer_efficiency = 1.0 - np.mean(buffer_usage)  # Lower buffer usage is better
        
        return 0.7 * throughput_efficiency + 0.3 * buffer_efficiency
        
    def _calculate_adaptation_speed(self, metrics: List[PerformanceMetrics]) -> float:
        """Calculate how quickly the scheduler adapts to changes"""
        if len(metrics) < 2:
            return 0.0
            
        # Calculate rate of change in throughput
        throughput_changes = np.diff([m.throughput for m in metrics])
        return np.mean(np.abs(throughput_changes))
        
    def _suggest_improvements(self, weaknesses: List[str]) -> List[str]:
        """Suggest improvements based on identified weaknesses"""
        suggestions = []
        for weakness in weaknesses:
            if "High latency" in weakness:
                suggestions.append("Consider implementing predictive path selection")
            elif "Unstable throughput" in weakness:
                suggestions.append("Implement better congestion control mechanisms")
            # Add more suggestions based on other weaknesses
        return suggestions
        
    def _determine_use_cases(self, strengths: List[str]) -> List[str]:
        """Determine best use cases based on strengths"""
        use_cases = []
        if "Low latency" in strengths:
            use_cases.append("Real-time applications")
        if "Stable throughput" in strengths:
            use_cases.append("Streaming media")
        # Add more use cases based on other strengths
        return use_cases
###########################################
# Part 4: Testing Framework and Visualization
###########################################

class NetworkTestFramework:
    """Enhanced network testing framework with virtual topology simulation"""
    
    def __init__(self, schedulers: Dict, monitor_service: Dict):
        self.schedulers = schedulers
        self.monitor_service = monitor_service
        self.topology = None
        self.simulator = None
        self.test_scenarios = self._init_test_scenarios()
        self.results_repository = defaultdict(dict)
        self.visualization_engine = VisualizationEngine()
        self.n_paths = 4
        
    def _init_test_scenarios(self) -> Dict:
        """Initialize test scenarios configuration"""
        return {
            NetworkCondition.IDEAL: {
                'latency': [10, 20],  # ms
                'bandwidth': [100, 150],  # Mbps
                'loss': [0.0, 0.0],  # %
                'duration': 60  # seconds
            },
            NetworkCondition.HIGH_LATENCY: {
                'latency': [100, 200],
                'bandwidth': [100, 150],
                'loss': [0.1, 0.1],
                'duration': 60
            },
            NetworkCondition.PACKET_LOSS: {
                'latency': [20, 30],
                'bandwidth': [100, 150],
                'loss': [2.0, 5.0],
                'duration': 60
            },
            NetworkCondition.BANDWIDTH_FLUCTUATION: {
                'latency': [20, 30],
                'bandwidth': {'min': 50, 'max': 200, 'interval': 5},
                'loss': [0.1, 0.1],
                'duration': 120
            },
            NetworkCondition.CONGESTION: {
                'latency': [30, 50],
                'bandwidth': [100, 150],
                'loss': [1.0, 2.0],
                'cross_traffic': {'enabled': True, 'intensity': 0.8},
                'duration': 90
            },
            NetworkCondition.ASYMMETRIC: {
                'latency': [10, 100],
                'bandwidth': [200, 50],
                'loss': [0.1, 1.0],
                'duration': 60
            },
            NetworkCondition.INTERMITTENT: {
                'latency': [20, 30],
                'bandwidth': [100, 150],
                'loss': {'pattern': 'burst', 'min': 0, 'max': 10, 'interval': 10},
                'duration': 180
            },
            NetworkCondition.MOBILE: {
                'latency': {'pattern': 'random_walk', 'min': 20, 'max': 200},
                'bandwidth': {'pattern': 'random_walk', 'min': 50, 'max': 150},
                'loss': {'pattern': 'random_walk', 'min': 0, 'max': 5},
                'duration': 300
            }
        }

    def setup_network(self, topo_type='fat-tree'):
        """Set up the virtual network topology"""
        if topo_type == 'fat-tree':
            self.topology = FatTreeTopology(k=4)  # Smaller size for testing
        else:
            self.topology = JellyfishTopology(n_switches=16, n_ports=4, n_hosts=16)
            
        self.simulator = NetworkSimulator(self.topology)
        
    def generate_traffic(self, src_node: str, dst_node: str, flow_size: int) -> Dict:
        """Generate simulated traffic between nodes"""
        return self.simulator.simulate_flow(src_node, dst_node, flow_size)

    
    def run_comprehensive_tests(self):
        """Run tests with both topologies and traffic patterns"""
        for topo_type in ['fat-tree', 'jellyfish']:
            print(f"\nTesting with {topo_type} topology...")
            
            # Setup network
            self.setup_network(topo_type)
            
            for condition in NetworkCondition:
                print(f"\nTesting {condition.value} conditions...")
                
                # Get random source and destination nodes
                hosts = [node_id for node_id, node in self.topology.nodes.items() 
                        if node.type == 'host']
                
                # Generate both elephant and mice flows
                for flow_type in ['elephant', 'mice']:
                    src_host = random.choice(hosts)
                    dst_host = random.choice([h for h in hosts if h != src_host])
                    
                    flow_size = random.randint(100000000, 1000000000) if flow_type == 'elephant' \
                               else random.randint(1000, 10000)
                    
                    # Run condition tests with this flow
                    self.results_repository[condition].update(
                        self._run_condition_tests(condition, src_host, dst_host, flow_size)
                    )
                    
        self._generate_test_report()
        
    def _run_condition_tests(self, condition: NetworkCondition, 
                           src_host: str, dst_host: str, flow_size: int) -> Dict:
        """Run tests for specific network condition"""
        condition_results = {}
        config = self.test_scenarios[condition]
        
        for name, scheduler in self.schedulers.items():
            print(f"Testing {name} scheduler...")
            
            # Start monitoring
            self.monitor_service[name].start_monitoring()
            
            # Configure network conditions
            self._configure_network(config)
            
            # Run test workload
            test_results = self._run_test_workload(
                scheduler, config, src_host, dst_host, flow_size
            )
            
            # Stop monitoring
            self.monitor_service[name].stop_monitoring()
            
            # Store results
            condition_results[name] = {
                'metrics': test_results,
                'monitor_data': self.monitor_service[name].metrics_history
            }
            
        return condition_results
    
    
    def _configure_network(self, config: Dict):
        """Configure network conditions based on test configuration"""
        try:
            for link in self.topology.links:
                # Set latency
                if isinstance(config['latency'], list):
                    link.latency = random.uniform(
                        config['latency'][0], 
                        config['latency'][1]
                    )
                elif isinstance(config['latency'], dict):
                    if config['latency']['pattern'] == 'random_walk':
                        link.latency = random.uniform(
                            config['latency']['min'],
                            config['latency']['max']
                        )
                        
                # Set bandwidth
                if isinstance(config['bandwidth'], list):
                    link.bandwidth = random.uniform(
                        config['bandwidth'][0],
                        config['bandwidth'][1]
                    )
                elif isinstance(config['bandwidth'], dict):
                    if 'pattern' in config['bandwidth']:
                        link.bandwidth = random.uniform(
                            config['bandwidth']['min'],
                            config['bandwidth']['max']
                        )
                        
                # Set packet loss
                if isinstance(config['loss'], list):
                    link.packet_loss = config['loss'][0] / 100.0
                elif isinstance(config['loss'], dict):
                    if config['loss']['pattern'] == 'burst':
                        link.packet_loss = random.uniform(
                            config['loss']['min'],
                            config['loss']['max']
                        ) / 100.0
                        
                # Configure cross traffic if enabled
                if 'cross_traffic' in config and config['cross_traffic']['enabled']:
                    link.current_load = link.bandwidth * config['cross_traffic']['intensity']
                    
        except Exception as e:
            print(f"Error configuring network: {str(e)}")
            
    def _run_test_workload(self, scheduler, config: Dict, 
                          src_host: str, dst_host: str, flow_size: int) -> Dict:
        """Run test workload and collect metrics"""
        try:
            metrics = {
                'latency': {'values': [], 'mean': 0, 'std': 0},
                'throughput': {'values': [], 'mean': 0, 'std': 0},
                'packet_loss': {'values': [], 'mean': 0, 'std': 0},
                'path_utilization': {'values': [], 'mean': 0, 'std': 0}
            }
            
            # Get available paths between source and destination
            paths = self.simulator.get_paths(src_host, dst_host, k=self.n_paths)
            
            # Convert paths to PathState objects for the scheduler
            path_states = {}
            for i, path in enumerate(paths):
                path_metrics = self._calculate_path_metrics(path)
                path_states[f'path{i}'] = PathState(
                    id=f'path{i}',
                    rtt=path_metrics['latency'] * 2,  # RTT is 2x latency
                    bandwidth=path_metrics['bandwidth'],
                    cwnd=1000,
                    congestion_level=path_metrics['congestion'],
                    available_buffer=64000,
                    in_recovery=False,
                    last_sent_time=0.0
                )
            
            # Update scheduler with available paths
            scheduler.paths = path_states
            
            # Run simulation for configured duration
            duration = config.get('duration', 60)
            start_time = time.time()
            packet_id = 0
            
            while time.time() - start_time < duration:
                # Generate test packet
                packet = Packet(
                    id=packet_id,
                    size=1500,  # Standard MTU size
                    stream_id="test_stream",
                    deadline=time.time() + 0.1,  # 100ms deadline
                    priority=1,
                    creation_time=time.time()
                )
                
                # Get scheduler's path selection
                selected_path = scheduler.select_path(packet)
                
                if selected_path:
                    # Get actual path from topology
                    path_index = int(selected_path.id.replace('path', ''))
                    actual_path = paths[path_index]
                    
                    # Simulate flow on selected path
                    flow_result = self.generate_traffic(
                        src_host, dst_host, packet.size
                    )
                    
                    if flow_result:
                        # Record metrics
                        metrics['latency']['values'].append(flow_result['latency'])
                        metrics['throughput']['values'].append(flow_result['bandwidth'])
                        metrics['packet_loss']['values'].append(flow_result['packet_loss'])
                        
                        # Calculate path utilization
                        utilization = self._calculate_path_utilization(actual_path)
                        metrics['path_utilization']['values'].append(utilization)
                
                packet_id += 1
                time.sleep(0.001)  # Small delay between packets
            
            # Calculate statistics
            for metric in metrics.keys():
                values = metrics[metric]['values']
                if values:
                    metrics[metric]['mean'] = np.mean(values)
                    metrics[metric]['std'] = np.std(values)
            
            return metrics
            
        except Exception as e:
            print(f"Error running test workload: {str(e)}")
            return metrics
            
    def _calculate_path_metrics(self, path: List[str]) -> Dict:
        """Calculate metrics for a given path"""
        total_latency = 0
        min_bandwidth = float('inf')
        total_loss = 0
        total_congestion = 0
        
        for i in range(len(path) - 1):
            link = self.simulator._get_link(path[i], path[i+1])
            if link:
                total_latency += link.latency
                min_bandwidth = min(min_bandwidth, link.bandwidth - link.current_load)
                total_loss += link.packet_loss
                total_congestion += link.current_load / link.bandwidth
        
        return {
            'latency': total_latency,
            'bandwidth': min_bandwidth if min_bandwidth != float('inf') else 0,
            'packet_loss': total_loss,
            'congestion': total_congestion / (len(path) - 1) if len(path) > 1 else 0
        }
        
    def _calculate_path_utilization(self, path: List[str]) -> float:
        """Calculate current utilization of a path"""
        utilization = 0
        for i in range(len(path) - 1):
            link = self.simulator._get_link(path[i], path[i+1])
            if link:
                utilization += link.current_load / link.bandwidth
        return utilization / (len(path) - 1) if len(path) > 1 else 0

    def _generate_test_report(self):
        """Generate comprehensive test report"""
        analyzer = PerformanceAnalyzer(self.monitor_service)
        analyzer.analyze_all()
        
        # Generate visualizations
        self.visualization_engine.generate_all_plots(
            results = self.results_repository,
            analysis = analyzer.comparison_results
        )
        
        # Save results
        self._save_test_results()
        
    def _save_test_results(self):
        """Save the test results to a text file."""
        try:
            results_file = "test_results.txt"
            with open(results_file, "w") as f:
                for condition, schedulers_results in self.results_repository.items():
                    f.write(f"Condition: {condition.value}\n")
                    for scheduler, results in schedulers_results.items():
                        f.write(f"  Scheduler: {scheduler}\n")
                        f.write(f"    Metrics:\n")
                        for metric, data in results['metrics'].items():
                            f.write(f"      {metric.capitalize()}:\n")
                            for stat, value in data.items():
                                f.write(f"        {stat}: {value}\n")
                        f.write(f"    Monitor Data:\n")
                        for entry in results['monitor_data']:
                            f.write(f"      {entry}\n")
                    f.write("\n")
            print(f"Test results saved to {results_file}.")
        except Exception as e:
            print(f"Error saving test results: {e}")

    def _average_metrics(self, metrics_list: List[Dict]) -> Dict:
    """Average multiple sets of metrics"""
    if not metrics_list:
        return {}

    # Initialize result dictionary with the same structure as input metrics
    result = {
        'latency': {'mean': 0.0, 'std': 0.0},
        'throughput': {'mean': 0.0, 'std': 0.0},
        'packet_loss': {'mean': 0.0, 'std': 0.0},
        'path_utilization': {'mean': 0.0, 'std': 0.0}
    }

    # Calculate mean for each metric
    for metric_name in result.keys():
        means = [m[metric_name]['mean'] for m in metrics_list if metric_name in m]
        stds = [m[metric_name]['std'] for m in metrics_list if metric_name in m]
        
        if means:
            result[metric_name]['mean'] = np.mean(means)
            result[metric_name]['std'] = np.mean(stds)

    return result

    def run_expanded_tests(self):
    """Run comprehensive tests for each topology, including all test and multipath scenarios"""
    results = {
        'fat-tree': {
            'test_scenarios': {},
            'multipath_scenarios': {},
            'overall': {}
        },
        'jellyfish': {
            'test_scenarios': {},
            'multipath_scenarios': {},
            'overall': {}
        }
    }
    
    for topo_type in ['fat-tree', 'jellyfish']:
        print(f"\nTesting with {topo_type} topology...")
        self.setup_network(topo_type)
        
        # Test Network Conditions
        for condition in NetworkCondition:
            print(f"\nTesting network condition: {condition.value}")
            results[topo_type]['test_scenarios'][condition] = \
                self._run_network_condition_tests(condition)
                
        # Test Multipath Scenarios
        for scenario in MultipathScenario:
            print(f"\nTesting multipath scenario: {scenario.value}")
            results[topo_type]['multipath_scenarios'][scenario] = \
                self._run_multipath_scenario_tests(scenario)
                
        # Calculate overall metrics for this topology
        results[topo_type]['overall'] = self._calculate_overall_metrics(
            results[topo_type]['test_scenarios'],
            results[topo_type]['multipath_scenarios']
        )
        
    self._generate_expanded_report(results)
    return results

    def _run_network_condition_tests(self, condition: NetworkCondition) -> Dict:
        """Run tests for specific network condition"""
        condition_results = {}
        config = self.test_scenarios[condition]
    
        # Get random source and destination hosts
        hosts = [node_id for node_id, node in self.topology.nodes.items() 
                if node.type == 'host']
    
        for name, scheduler in self.schedulers.items():
            print(f"Testing {name} scheduler...")
            metrics = []
        
            # Run multiple iterations for statistical significance
            for _ in range(3):  # Run 3 iterations
                src_host = random.choice(hosts)
                dst_host = random.choice([h for h in hosts if h != src_host])
            
                # Start monitoring
                self.monitor_service[name].start_monitoring()
            
                # Configure network conditions
                self._configure_network(config)
            
                # Run test workload
                test_results = self._run_test_workload(
                scheduler, config, src_host, dst_host, 
                random.randint(1000000, 10000000)  # Random flow size
                )
            
                # Stop monitoring
                self.monitor_service[name].stop_monitoring()
                metrics.append(test_results)
            
            # Average the metrics
            condition_results[name] = self._average_metrics(metrics)
        
        return condition_results

    def _run_multipath_scenario_tests(self, scenario: MultipathScenario) -> Dict:
        """Run tests for specific multipath scenario"""
        scenario_results = {}
    
        for name, scheduler in self.schedulers.items():
            print(f"Testing {name} scheduler...")
            metrics = []
        
            # Configure scenario-specific parameters
            self._configure_multipath_scenario(scenario)
        
            # Run multiple iterations
            for _ in range(3):
                test_results = self._run_scenario_workload(scheduler, scenario)
                metrics.append(test_results)
            
                scenario_results[name] = self._average_metrics(metrics)
        
        return scenario_results

    def _calculate_overall_metrics(self, test_results: Dict, scenario_results: Dict) -> Dict:
        """Calculate overall performance metrics for each scheduler"""
        overall_metrics = {}
    
        for scheduler_name in self.schedulers.keys():
            # Initialize metrics
            metrics_sum = {
                'latency': 0.0,
                'throughput': 0.0,
                'packet_loss': 0.0,
                'path_utilization': 0.0,
                'adaptation_speed': 0.0,
                'fairness': 0.0,
                'stability': 0.0
            }
        
            # Sum up metrics from test scenarios
            for condition_results in test_results.values():
                for metric, value in condition_results[scheduler_name].items():
                    if isinstance(value, dict) and 'mean' in value:
                        metrics_sum[metric] += value['mean']
                    
            # Sum up metrics from multipath scenarios
            for scenario_results in scenario_results.values():
                for metric, value in scenario_results[scheduler_name].items():
                    if isinstance(value, dict) and 'mean' in value:
                    metrics_sum[metric] += value['mean']
                    
            # Calculate averages
            total_scenarios = len(NetworkCondition) + len(MultipathScenario)
            overall_metrics[scheduler_name] = {
                metric: value / total_scenarios
                for metric, value in metrics_sum.items()
            }
        
        return overall_metrics

    def _generate_expanded_report(self, results: Dict):
        """Generate comprehensive report including all scenarios and topologies"""
        # Create visualization engine instance
        viz = ExpandedVisualizationEngine()
    
        # Generate visualizations
        viz.generate_topology_comparison(results)
        viz.generate_scenario_comparison(results)
        viz.generate_multipath_analysis(results)
        viz.generate_overall_performance(results)
    
        # Generate detailed HTML report
        report_generator = DetailedReportGenerator()
        report_generator.generate_report(results)
            
class MultipathTestScenarios:
    def __init__(self, network, schedulers: Dict, monitor_service):
        self.network = network
        self.schedulers = schedulers
        self.monitor_service = monitor_service
        self.results = defaultdict(dict)
        self.path_configurations = self._init_path_configurations()
        
    def _init_path_configurations(self) -> Dict:
        """Initialize different multipath configurations"""
        return {
            MultipathScenario.DISJOINT_PATHS: {
                'paths': [
                    {'latency': '10ms', 'bandwidth': 100, 'loss': 0.1},
                    {'latency': '20ms', 'bandwidth': 150, 'loss': 0.2},
                    {'latency': '15ms', 'bandwidth': 120, 'loss': 0.15}
                ],
                'shared_links': []
            },
            MultipathScenario.SHARED_BOTTLENECK: {
                'paths': [
                    {'latency': '15ms', 'bandwidth': 100, 'loss': 0.1},
                    {'latency': '15ms', 'bandwidth': 100, 'loss': 0.1}
                ],
                'bottleneck': {'bandwidth': 150, 'buffer_size': 1000}
            },
            MultipathScenario.PATH_DIVERSITY: {
                'paths': [
                    {'latency': '5ms', 'bandwidth': 50, 'loss': 0.1},
                    {'latency': '20ms', 'bandwidth': 200, 'loss': 0.5},
                    {'latency': '100ms', 'bandwidth': 1000, 'loss': 1.0}
                ]
            },
            MultipathScenario.DYNAMIC_PATH: {
                'initial_paths': [
                    {'latency': '10ms', 'bandwidth': 100, 'loss': 0.1},
                    {'latency': '20ms', 'bandwidth': 150, 'loss': 0.2}
                ],
                'path_changes': [
                    {'time': 30, 'action': 'add', 'path': {'latency': '15ms', 'bandwidth': 120, 'loss': 0.15}},
                    {'time': 60, 'action': 'remove', 'path_index': 0},
                    {'time': 90, 'action': 'modify', 'path_index': 1, 'new_params': {'bandwidth': 200}}
                ]
            },
            MultipathScenario.HETEROGENEOUS_PATHS: {
                'paths': [
                    {'latency': '5ms', 'bandwidth': 1000, 'loss': 0.1, 'type': 'fiber'},
                    {'latency': '20ms', 'bandwidth': 100, 'loss': 0.5, 'type': 'wifi'},
                    {'latency': '50ms', 'bandwidth': 50, 'loss': 1.0, 'type': 'cellular'}
                ]
            },
            MultipathScenario.OVERLAPPING_PATHS: {
                'paths': [
                    {'segments': [('A', 'B', 10), ('B', 'C', 20)]},
                    {'segments': [('A', 'B', 10), ('B', 'D', 15), ('D', 'C', 10)]},
                    {'segments': [('A', 'E', 15), ('E', 'C', 20)]}
                ]
            },
            MultipathScenario.BACKUP_PATH: {
                'primary_path': {'latency': '10ms', 'bandwidth': 200, 'loss': 0.1},
                'backup_paths': [
                    {'latency': '20ms', 'bandwidth': 100, 'loss': 0.2},
                    {'latency': '30ms', 'bandwidth': 50, 'loss': 0.3}
                ],
                'failure_scenarios': [
                    {'time': 50, 'duration': 20, 'type': 'complete'},
                    {'time': 100, 'duration': 30, 'type': 'degradation'}
                ]
            },
            MultipathScenario.LOAD_BALANCING: {
                'paths': [
                    {'latency': '10ms', 'bandwidth': 100, 'loss': 0.1},
                    {'latency': '10ms', 'bandwidth': 100, 'loss': 0.1},
                    {'latency': '10ms', 'bandwidth': 100, 'loss': 0.1}
                ],
                'load_patterns': [
                    {'time': 0, 'distribution': [0.33, 0.33, 0.34]},
                    {'time': 50, 'distribution': [0.5, 0.25, 0.25]},
                    {'time': 100, 'distribution': [0.2, 0.4, 0.4]}
                ]
            }
        }
        
class MultipathAnalysis:
    
    def __init__(self, test_results: Dict):
        self.results = test_results
        self.metrics = self._initialize_metrics()
        
    def _initialize_metrics(self) -> Dict:
        return {
            'path_utilization': defaultdict(dict),
            'load_distribution': defaultdict(dict),
            'failover_performance': defaultdict(dict),
            'adaptation_speed': defaultdict(dict),
            'path_correlation': defaultdict(dict)
        }
        
    def analyze_path_utilization(self) -> Dict:
        """Analyze how effectively each scheduler utilizes multiple paths"""
        utilization_metrics = {}
        
        for scenario in MultipathScenario:
            scenario_results = self.results[scenario]
            for scheduler, metrics in scenario_results.items():
                path_usage = self._calculate_path_usage(metrics)
                utilization_metrics[scheduler] = {
                    'efficiency': np.mean(path_usage),
                    'balance': np.std(path_usage),
                    'peak_utilization': np.max(path_usage)
                }
                
        return utilization_metrics
        
    def analyze_load_balancing(self) -> Dict:
        """Analyze load balancing effectiveness"""
        load_metrics = {}
        
        for scheduler, metrics in self.results[MultipathScenario.LOAD_BALANCING].items():
            load_distribution = self._calculate_load_distribution(metrics)
            load_metrics[scheduler] = {
                'fairness_index': self._calculate_jain_fairness(load_distribution),
                'stability': self._calculate_distribution_stability(load_distribution),
                'adaptation_time': self._calculate_adaptation_time(load_distribution)
            }
            
        return load_metrics
        
    def analyze_failover_behavior(self) -> Dict:
        """Analyze failover performance"""
        failover_metrics = {}
        
        backup_results = self.results[MultipathScenario.BACKUP_PATH]
        for scheduler, metrics in backup_results.items():
            failover_metrics[scheduler] = {
                'detection_time': self._calculate_detection_time(metrics),
                'recovery_time': self._calculate_recovery_time(metrics),
                'throughput_drop': self._calculate_throughput_impact(metrics)
            }
            
        return failover_metrics
        
    def analyze_path_diversity(self) -> Dict:
        """Analyze how schedulers handle path diversity"""
        diversity_metrics = {}
        
        diversity_results = self.results[MultipathScenario.PATH_DIVERSITY]
        for scheduler, metrics in diversity_results.items():
            diversity_metrics[scheduler] = {
                'path_selection_efficiency': self._calculate_selection_efficiency(metrics),
                'adaptation_to_diversity': self._calculate_diversity_adaptation(metrics),
                'performance_stability': self._calculate_performance_stability(metrics)
            }
            
        return diversity_metrics

class MultipathVisualization:
    def __init__(self, analysis_results: Dict):
        self.results = analysis_results
        
    def plot_path_utilization(self, save_path: str = None):
        """Plot path utilization patterns"""
        plt.figure(figsize=(12, 6))
        data = self.results['path_utilization']
        
        schedulers = list(data.keys())
        efficiency = [d['efficiency'] for d in data.values()]
        balance = [d['balance'] for d in data.values()]
        
        x = np.arange(len(schedulers))
        width = 0.35
        
        plt.bar(x - width/2, efficiency, width, label='Efficiency')
        plt.bar(x + width/2, balance, width, label='Balance')
        
        plt.xlabel('Schedulers')
        plt.ylabel('Utilization Metrics')
        plt.title('Path Utilization by Scheduler')
        plt.xticks(x, schedulers, rotation=45)
        plt.legend()
        
        if save_path:
            plt.savefig(save_path)
        plt.close()
        
    def plot_load_distribution(self, save_path: str = None):
        """Plot load distribution patterns"""
        plt.figure(figsize=(12, 6))
        data = self.results['load_distribution']
        
        for scheduler, distributions in data.items():
            plt.plot(distributions['time_series'], label=scheduler)
            
        plt.xlabel('Time (s)')
        plt.ylabel('Load Distribution')
        plt.title('Load Distribution Over Time')
        plt.legend()
        
        if save_path:
            plt.savefig(save_path)
        plt.close()

def run_multipath_tests():
    """Run comprehensive multipath tests"""
    test_scenarios = MultipathTestScenarios(network, schedulers, monitor_service)
    
    for scenario in MultipathScenario:
        print(f"\nTesting {scenario.value} scenario...")
        for scheduler_name, scheduler in schedulers.items():
            # Configure network for scenario
            test_scenarios._configure_scenario(scenario)
            
            # Run test
            results = test_scenarios._run_scenario_test(
                scenario, scheduler_name, scheduler
            )
            
            # Store results
            test_scenarios.results[scenario][scheduler_name] = results
            
    # Analyze results
    analysis = MultipathAnalysis(test_scenarios.results)
    
    # Generate visualizations
    visualization = MultipathVisualization(analysis.results)
    visualization.plot_path_utilization('path_utilization.png')
    visualization.plot_load_distribution('load_distribution.png')
    
    return {
        'raw_results': test_scenarios.results,
        'analysis': analysis.results,
        'plots': {
            'path_utilization': 'path_utilization.png',
            'load_distribution': 'load_distribution.png'
        }
    }
            
    def run_expanded_tests(self):
        """Run comprehensive tests for each topology, including all test and multipath scenarios"""
        results = {
            'fat-tree': {
                'test_scenarios': {},
                'multipath_scenarios': {},
                'overall': {}
            },
            'jellyfish': {
                'test_scenarios': {},
                'multipath_scenarios': {},
                'overall': {}
            }
        }
    
        for topo_type in ['fat-tree', 'jellyfish']:
            print(f"\nTesting with {topo_type} topology...")
            self.setup_network(topo_type)
        
        # Test Network Conditions
        for condition in NetworkCondition:
            print(f"\nTesting network condition: {condition.value}")
            results[topo_type]['test_scenarios'][condition] = \
                self._run_network_condition_tests(condition)
                
        # Test Multipath Scenarios
        for scenario in MultipathScenario:
            print(f"\nTesting multipath scenario: {scenario.value}")
            results[topo_type]['multipath_scenarios'][scenario] = \
                self._run_multipath_scenario_tests(scenario)
                
        # Calculate overall metrics for this topology
        results[topo_type]['overall'] = self._calculate_overall_metrics(
            results[topo_type]['test_scenarios'],
            results[topo_type]['multipath_scenarios']
        )
        
        self._generate_expanded_report(results)
        return results

    def _run_network_condition_tests(self, condition: NetworkCondition) -> Dict:
        """Run tests for specific network condition"""
        condition_results = {}
        config = self.test_scenarios[condition]
    
        # Get random source and destination hosts
        hosts = [node_id for node_id, node in self.topology.nodes.items() 
                if node.type == 'host']
    
        for name, scheduler in self.schedulers.items():
            print(f"Testing {name} scheduler...")
            metrics = []
        
        # Run multiple iterations for statistical significance
        for _ in range(3):  # Run 3 iterations
            src_host = random.choice(hosts)
            dst_host = random.choice([h for h in hosts if h != src_host])
            
            # Start monitoring
            self.monitor_service[name].start_monitoring()
            
            # Configure network conditions
            self._configure_network(config)
            
            # Run test workload
            test_results = self._run_test_workload(
                scheduler, config, src_host, dst_host, 
                random.randint(1000000, 10000000)  # Random flow size
            )
            
            # Stop monitoring
            self.monitor_service[name].stop_monitoring()
            metrics.append(test_results)
            
            # Average the metrics
            condition_results[name] = self._average_metrics(metrics)
        
        return condition_results

    def _run_multipath_scenario_tests(self, scenario: MultipathScenario) -> Dict:
        """Run tests for specific multipath scenario"""
        scenario_results = {}
    
        for name, scheduler in self.schedulers.items():
            print(f"Testing {name} scheduler...")
            metrics = []
        
            # Configure scenario-specific parameters
            self._configure_multipath_scenario(scenario)
        
            # Run multiple iterations
            for _ in range(3):
                test_results = self._run_scenario_workload(scheduler, scenario)
                metrics.append(test_results)
            
                scenario_results[name] = self._average_metrics(metrics)
        
        return scenario_results

    def _calculate_overall_metrics(self, test_results: Dict, scenario_results: Dict) -> Dict:
        """Calculate overall performance metrics for each scheduler"""
        overall_metrics = {}
    
        for scheduler_name in self.schedulers.keys():
            # Initialize metrics
            metrics_sum = {
                'latency': 0.0,
                'throughput': 0.0,
                'packet_loss': 0.0,
                'path_utilization': 0.0,
                'adaptation_speed': 0.0,
                'fairness': 0.0,
                'stability': 0.0
            }
        
            # Sum up metrics from test scenarios
            for condition_results in test_results.values():
                for metric, value in condition_results[scheduler_name].items():
                    if isinstance(value, dict) and 'mean' in value:
                        metrics_sum[metric] += value['mean']
                    
            # Sum up metrics from multipath scenarios
            for scenario_results in scenario_results.values():
                for metric, value in scenario_results[scheduler_name].items():
                    if isinstance(value, dict) and 'mean' in value:
                        metrics_sum[metric] += value['mean']
                    
            # Calculate averages
            total_scenarios = len(NetworkCondition) + len(MultipathScenario)
            overall_metrics[scheduler_name] = {
                metric: value / total_scenarios
                for metric, value in metrics_sum.items()
            }
        
        return overall_metrics

    def _generate_expanded_report(self, results: Dict):
        """Generate comprehensive report including all scenarios and topologies"""
        # Create visualization engine instance
        viz = ExpandedVisualizationEngine()
    
        # Generate visualizations
        viz.generate_topology_comparison(results)
        viz.generate_scenario_comparison(results)
        viz.generate_multipath_analysis(results)
        viz.generate_overall_performance(results)
    
        # Generate detailed HTML report
        report_generator = DetailedReportGenerator()
        report_generator.generate_report(results)

class VisualizationEngine:
    """Engine for generating visualizations of test results"""
    
    def __init__(self):
        self.style_config = {
            'figure.figsize': (12, 6),
            'axes.titlesize': 14,
            'axes.labelsize': 12,
            'lines.linewidth': 2,
            'lines.markersize': 8,
            'xtick.labelsize': 10,
            'ytick.labelsize': 10
        }
        self.color_palette = sns.color_palette("husl", 8)
        
    def generate_all_plots(self, results: Dict, analysis: Dict):
        """Generate all visualization plots"""
        try:
            plt.style.use('seaborn')
        except OSError:
            plt.style.use('default')  # Fallback to default matplotlib style
            
        for key, value in self.style_config.items():
            plt.rcParams[key] = value
            
        # Make sure to pass results to each plotting method
        self._plot_performance_comparison(results)
        self._plot_stability_analysis(analysis)
        self._plot_efficiency_metrics(analysis)
        self._generate_heatmap(results)
        self._plot_time_series(results)
        self._create_dashboard(results, analysis)
        
    def _plot_performance_comparison(self, results: Dict):
        """Plot performance comparison across schedulers"""
        plt.figure(figsize=(12, 6))
        
        # Get first network condition to get list of schedulers
        first_condition = list(results.keys())[0]
        metrics = ['latency', 'throughput', 'packet_loss', 'path_utilization']
        schedulers = list(results[first_condition].keys())
        
        x = np.arange(len(metrics))
        width = 0.8 / len(schedulers)
        
        for i, scheduler in enumerate(schedulers):
            values = []
            for metric in metrics:
                # Normalize metric values
                values.append(np.mean([
                    result[scheduler]['metrics'][metric]['mean']
                    for result in results.values()
                ]))
            
            plt.bar(x + i * width, values, width, 
                   label=scheduler, color=self.color_palette[i])
            
        plt.xlabel('Metrics')
        plt.ylabel('Normalized Value')
        plt.title('Performance Comparison Across Schedulers')
        plt.xticks(x + width * (len(schedulers) - 1) / 2, metrics)
        plt.legend()
        plt.tight_layout()
        plt.savefig('performance_comparison.png')
        plt.close()
            
    def _plot_stability_analysis(self, analysis: Dict):
        """Plot stability analysis results"""
        plt.figure(figsize=(10, 6))
        
        stability_data = analysis['stability_analysis']
        schedulers = list(stability_data.keys())
        metrics = ['throughput_stability', 'latency_stability', 'overall_stability']
        
        data = np.array([[stability_data[s][m] for m in metrics] for s in schedulers])
        
        sns.heatmap(data, annot=True, fmt='.2f', 
                   xticklabels=metrics, yticklabels=schedulers,
                   cmap='YlOrRd', center=0.5)
        
        plt.title('Stability Analysis Heatmap')
        plt.tight_layout()
        plt.savefig('stability_analysis.png')
        plt.close()
        
    def _plot_efficiency_metrics(self, analysis: Dict):
        """Plot efficiency metrics"""
        efficiency_data = analysis['efficiency_analysis']
        
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # Path Efficiency
        schedulers = list(efficiency_data.keys())
        path_eff = [efficiency_data[s]['path_efficiency'] for s in schedulers]
        
        ax1.bar(schedulers, path_eff, color=self.color_palette)
        ax1.set_title('Path Efficiency')
        ax1.set_ylim(0, 1)
        ax1.tick_params(axis='x', rotation=45)
        
        # Resource Utilization
        resource_util = [efficiency_data[s]['resource_utilization'] 
                        for s in schedulers]
        
        ax2.bar(schedulers, resource_util, color=self.color_palette)
        ax2.set_title('Resource Utilization')
        ax2.set_ylim(0, 1)
        ax2.tick_params(axis='x', rotation=45)
        
        plt.tight_layout()
        plt.savefig('efficiency_metrics.png')
        plt.close()
        
    
    def _generate_heatmap(self, results: Dict):
        """Generate heatmap of performance across conditions"""
        # Get list of conditions and schedulers
        first_condition = list(results.keys())[0]
        conditions = list(results.keys())
        schedulers = list(results[first_condition].keys())
        
        data = np.zeros((len(schedulers), len(conditions)))
        
        for i, scheduler in enumerate(schedulers):
            for j, condition in enumerate(conditions):
                metrics = results[condition][scheduler]['metrics']
                
                # Handle division by zero for latency
                latency_score = 1 / metrics['latency']['mean'] if metrics['latency']['mean'] > 0 else 0
                
                score = (
                    latency_score * 0.3 +  # Lower latency is better
                    metrics['throughput']['mean'] * 0.3 +  # Higher throughput is better
                    (1 - metrics['packet_loss']['mean']) * 0.2 +  # Lower packet loss is better
                    metrics['path_utilization']['mean'] * 0.2  # Higher utilization is better
                )
                data[i, j] = score
        
        plt.figure(figsize=(12, 8))
        sns.heatmap(data, annot=True, fmt='.2f',
                   xticklabels=[c.value for c in conditions],
                   yticklabels=schedulers,
                   cmap='viridis')
        
        plt.title('Performance Heatmap Across Network Conditions')
        plt.tight_layout()
        plt.savefig('performance_heatmap.png')
        plt.close()
        
    def _plot_time_series(self, results: Dict):
        """Plot time series data for key metrics"""
        for condition in results.keys():  # Changed from NetworkCondition to results.keys()
            condition_results = results[condition]
            
            fig, ax = plt.subplots(figsize=(15, 10))
            fig.suptitle(f'Time Series Analysis - {condition.value}')
            
            metrics = ['throughput', 'latency', 'packet_loss', 'path_utilization']
            
            for scheduler, data in condition_results.items():
                time_data = [m.timestamp for m in data['monitor_data']]
            
                for metric in metrics:
                    metric_data = [getattr(m, metric) for m in data['monitor_data']]
                    ax.plot(time_data, metric_data, 
                       label=f'{scheduler}-{metric}',
                       marker='o',
                       linestyle='-',
                       markersize=4)
                
        ax.set_xlabel('Time (s)')
        ax.set_ylabel('Metric Value')
        ax.set_title(f'Performance Metrics Over Time - {condition.value}')
        ax.grid(True)
        ax.legend()
        
        plt.tight_layout()
        plt.savefig(f'time_series_{condition.value}.png')
        plt.close()
        
    def _create_dashboard(self, results: Dict, analysis: Dict):
        """Create an HTML dashboard with all results"""
        html_content = self._generate_dashboard_html(results, analysis)
        
        with open('dashboard.html', 'w') as f:
            f.write(html_content)

    def _generate_dashboard_html(self, results: Dict, analysis: Dict) -> str:
        """Generate HTML content for dashboard"""
        # Previous implementation remains the same
        return f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Multipath Scheduler Analysis Dashboard</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .section {{ margin-bottom: 30px; }}
                .chart {{ margin: 20px 0; }}
                .metric-card {{ 
                    border: 1px solid #ddd; 
                    padding: 15px; 
                    margin: 10px; 
                    border-radius: 5px;
                }}
                .grid {{ 
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
                    gap: 20px;
                }}
            </style>
        </head>
        <body>
            <h1>Multipath Scheduler Analysis Dashboard</h1>
            
            <div class="section">
                <h2>Performance Overview</h2>
                <div class="grid">
                    <div class="metric-card">
                        <img src="performance_comparison.png" width="100%">
                    </div>
                    <div class="metric-card">
                        <img src="performance_heatmap.png" width="100%">
                    </div>
                </div>
            </div>
            
            <div class="section">
                <h2>Stability Analysis</h2>
                <div class="grid">
                    <div class="metric-card">
                        <img src="stability_analysis.png" width="100%">
                    </div>
                    <div class="metric-card">
                        <img src="efficiency_metrics.png" width="100%">
                    </div>
                </div>
            </div>
            
            <div class="section">
                <h2>Time Series Analysis</h2>
                <div class="grid">
                    {self._generate_time_series_html(results)}
                </div>
            </div>
            
            <div class="section">
                <h2>Recommendations</h2>
                {self._generate_recommendations_html(analysis)}
            </div>
        </body>
        </html>
        """
        
    def _generate_time_series_html(self, results: Dict) -> str:
        """Generate HTML for time series charts"""
        html = ""
        for condition in results.keys():  # Changed from NetworkCondition to results.keys()
            html += f"""
                <div class="metric-card">
                    <h3>{condition.value}</h3>
                    <img src="time_series_{condition.value}.png" width="100%">
                </div>
            """
        return html
        
    def _generate_recommendations_html(self, analysis: Dict) -> str:
        """Generate HTML for recommendations section"""
        html = "<div class='grid'>"
        for scheduler, data in analysis['recommendations'].items():
            html += f"""
                <div class="metric-card">
                    <h3>{scheduler}</h3>
                    <h4>Strengths:</h4>
                    <ul>
                        {"".join(f"<li>{s}</li>" for s in data['strengths'])}
                    </ul>
                    <h4>Suggested Improvements:</h4>
                    <ul>
                        {"".join(f"<li>{s}</li>" for s in data['suggested_improvements'])}
                    </ul>
                    <h4>Best Use Cases:</h4>
                    <ul>
                        {"".join(f"<li>{s}</li>" for s in data['best_use_cases'])}
                    </ul>
                </div>
            """
        html += "</div>"
        return html

class ExpandedVisualizationEngine:
    """Enhanced visualization engine for comprehensive scheduler analysis"""
    
    def __init__(self):
        self.style_config = {
            'figure.figsize': (15, 8),
            'axes.titlesize': 14,
            'axes.labelsize': 12,
            'lines.linewidth': 2,
            'lines.markersize': 8,
            'xtick.labelsize': 10,
            'ytick.labelsize': 10
        }
        # Extended color palette for more scenarios
        self.color_palette = sns.color_palette("husl", 16)
        plt.style.use('seaborn')
        for key, value in self.style_config.items():
            plt.rcParams[key] = value

    def generate_topology_comparison(self, results: Dict):
        """Generate comparison plots for different topologies"""
        # Create figure with subplots for different metrics
        metrics = ['latency', 'throughput', 'packet_loss', 'path_utilization']
        fig, axes = plt.subplots(2, 2, figsize=(20, 16))
        fig.suptitle('Topology Performance Comparison', fontsize=16)
        
        for idx, metric in enumerate(metrics):
            ax = axes[idx // 2, idx % 2]
            
            # Prepare data for plotting
            data = []
            labels = []
            colors = []
            
            for topo_type in ['fat-tree', 'jellyfish']:
                for scheduler in results[topo_type]['overall'].keys():
                    data.append(results[topo_type]['overall'][scheduler][metric])
                    labels.append(f"{scheduler}\n({topo_type})")
                    colors.append(self.color_palette[len(data) - 1])
            
            # Create bar plot
            bars = ax.bar(range(len(data)), data, color=colors)
            ax.set_xticks(range(len(data)))
            ax.set_xticklabels(labels, rotation=45, ha='right')
            ax.set_title(f'{metric.replace("_", " ").title()}')
            ax.grid(True, alpha=0.3)
            
            # Add value labels on bars
            for bar in bars:
                height = bar.get_height()
                ax.text(bar.get_x() + bar.get_width()/2., height,
                       f'{height:.2f}',
                       ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig('topology_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()

    def generate_scenario_comparison(self, results: Dict):
        """Generate comparison plots for different test scenarios"""
        for topo_type in ['fat-tree', 'jellyfish']:
            fig = plt.figure(figsize=(20, 12))
            gs = fig.add_gridspec(2, 2)
            fig.suptitle(f'Network Condition Scenarios - {topo_type.title()} Topology', 
                        fontsize=16)
            
            # Throughput subplot
            ax1 = fig.add_subplot(gs[0, 0])
            self._plot_scenario_metric(ax1, results[topo_type]['test_scenarios'],
                                    'throughput', 'Throughput (Mbps)')
            
            # Latency subplot
            ax2 = fig.add_subplot(gs[0, 1])
            self._plot_scenario_metric(ax2, results[topo_type]['test_scenarios'],
                                    'latency', 'Latency (ms)')
            
            # Packet Loss subplot
            ax3 = fig.add_subplot(gs[1, 0])
            self._plot_scenario_metric(ax3, results[topo_type]['test_scenarios'],
                                    'packet_loss', 'Packet Loss (%)')
            
            # Path Utilization subplot
            ax4 = fig.add_subplot(gs[1, 1])
            self._plot_scenario_metric(ax4, results[topo_type]['test_scenarios'],
                                    'path_utilization', 'Path Utilization (%)')
            
            plt.tight_layout()
            plt.savefig(f'scenario_comparison_{topo_type}.png', dpi=300, 
                       bbox_inches='tight')
            plt.close()

    def generate_multipath_analysis(self, results: Dict):
        """Generate analysis plots for multipath scenarios"""
        for topo_type in ['fat-tree', 'jellyfish']:
            # Create heatmap of performance across multipath scenarios
            scenario_data = results[topo_type]['multipath_scenarios']
            schedulers = list(scenario_data[list(scenario_data.keys())[0]].keys())
            scenarios = list(scenario_data.keys())
            
            # Prepare data for heatmap
            data = np.zeros((len(schedulers), len(scenarios)))
            for i, scheduler in enumerate(schedulers):
                for j, scenario in enumerate(scenarios):
                    # Combine metrics into a single score
                    metrics = scenario_data[scenario][scheduler]
                    score = (
                        (1 / metrics['latency']['mean'] if metrics['latency']['mean'] > 0 else 0) * 0.3 +
                        metrics['throughput']['mean'] * 0.3 +
                        (1 - metrics['packet_loss']['mean']) * 0.2 +
                        metrics['path_utilization']['mean'] * 0.2
                    )
                    data[i, j] = score
            
            plt.figure(figsize=(15, 10))
            sns.heatmap(data, annot=True, fmt='.2f',
                       xticklabels=[s.value for s in scenarios],
                       yticklabels=schedulers,
                       cmap='viridis')
            
            plt.title(f'Multipath Scenario Performance - {topo_type.title()} Topology')
            plt.xticks(rotation=45, ha='right')
            plt.tight_layout()
            plt.savefig(f'multipath_analysis_{topo_type}.png', dpi=300)
            plt.close()

    def generate_overall_performance(self, results: Dict):
        """Generate overall performance visualization"""
        # Create radar chart for overall performance
        schedulers = list(results['fat-tree']['overall'].keys())
        metrics = ['latency', 'throughput', 'packet_loss', 'path_utilization', 
                  'adaptation_speed', 'stability']
        
        # Number of variables
        num_vars = len(metrics)
        
        # Compute angle for each axis
        angles = [n / float(num_vars) * 2 * np.pi for n in range(num_vars)]
        angles += angles[:1]
        
        # Initialize the spider plot
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10), 
                                      subplot_kw=dict(projection='polar'))
        
        # Plot for Fat-tree topology
        self._plot_radar_chart(ax1, angles, metrics, schedulers, 
                             results['fat-tree']['overall'],
                             'Fat-tree Topology Performance')
        
        # Plot for Jellyfish topology
        self._plot_radar_chart(ax2, angles, metrics, schedulers,
                             results['jellyfish']['overall'],
                             'Jellyfish Topology Performance')
        
        plt.tight_layout()
        plt.savefig('overall_performance.png', dpi=300, bbox_inches='tight')
        plt.close()

    def _plot_scenario_metric(self, ax, scenario_data: Dict, metric: str, 
                            ylabel: str):
        """Helper method to plot scenario metrics"""
        scenarios = list(scenario_data.keys())
        schedulers = list(scenario_data[scenarios[0]].keys())
        
        x = np.arange(len(scenarios))
        width = 0.8 / len(schedulers)
        
        for i, scheduler in enumerate(schedulers):
            values = [scenario_data[scenario][scheduler][metric]['mean'] 
                     for scenario in scenarios]
            ax.bar(x + i * width, values, width, label=scheduler,
                  color=self.color_palette[i])
        
        ax.set_ylabel(ylabel)
        ax.set_xticks(x + width * (len(schedulers) - 1) / 2)
        ax.set_xticklabels([s.value for s in scenarios], rotation=45, ha='right')
        ax.legend()
        ax.grid(True, alpha=0.3)

    def _plot_radar_chart(self, ax, angles: List, metrics: List, 
                         schedulers: List, data: Dict, title: str):
        """Helper method to plot radar chart"""
        # Plot data
        for i, scheduler in enumerate(schedulers):
            values = [data[scheduler][metric] for metric in metrics]
            values += values[:1]
            
            ax.plot(angles, values, 'o-', linewidth=2, 
                   label=scheduler, color=self.color_palette[i])
            ax.fill(angles, values, alpha=0.25, color=self.color_palette[i])
        
        # Set chart properties
        ax.set_xticks(angles[:-1])
        ax.set_xticklabels(metrics)
        ax.set_title(title)
        ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))

    def generate_comparison_tables(self, results: Dict) -> str:
        """Generate HTML tables comparing scheduler performance"""
        html = "<div class='comparison-tables'>"
        
        # Generate tables for each topology
        for topo_type in ['fat-tree', 'jellyfish']:
            html += f"<h2>{topo_type.title()} Topology Results</h2>"
            
            # Overall metrics table
            html += self._generate_metric_table(
                results[topo_type]['overall'],
                f"Overall Performance - {topo_type.title()}"
            )
            
            # Scenario-specific tables
            html += "<h3>Network Condition Scenarios</h3>"
            for scenario in results[topo_type]['test_scenarios']:
                html += self._generate_metric_table(
                    results[topo_type]['test_scenarios'][scenario],
                    f"Scenario: {scenario.value}"
                )
        
        html += "</div>"
        return html

    def _generate_metric_table(self, data: Dict, title: str) -> str:
        """Helper method to generate HTML table for metrics"""
        html = f"<h4>{title}</h4>"
        html += "<table class='metric-table'>"
        
        # Header row
        html += "<tr><th>Scheduler</th>"
        metrics = ['latency', 'throughput', 'packet_loss', 'path_utilization']
        for metric in metrics:
            html += f"<th>{metric.replace('_', ' ').title()}</th>"
        html += "</tr>"
        
        # Data rows
        for scheduler, metrics_data in data.items():
            html += f"<tr><td>{scheduler}</td>"
            for metric in metrics:
                value = metrics_data[metric]['mean'] if isinstance(metrics_data[metric], dict) \
                        else metrics_data[metric]
                html += f"<td>{value:.2f}</td>"
            html += "</tr>"
        
        html += "</table>"
        return html
    
# MAIN Program Running
if __name__ == "__main__":
    # Create schedulers
    schedulers = {
        "ECF": ECFScheduler(),
        "RR": RRScheduler(),
        "MPQUIC": MPQUICScheduler(),
        "Peekabo": PeekaboScheduler(),
        "BLEST": BLESTScheduler(),
        "MAB": MABScheduler(),
        "MAMS": MAMSScheduler(),
    }

    # Create monitoring services for each scheduler
    monitor_service = create_monitoring_service()

    # Set up test paths for each scheduler
    test_paths = create_test_environment()
    for scheduler in schedulers.values():
        scheduler.paths = test_paths

    # Initialize the test framework
   
    framework = NetworkTestFramework(schedulers, monitor_service)

    # Run expanded tests with detailed topology and scenario analysis
    results = framework.run_expanded_tests()
   
    
    # Run comprehensive tests across all conditions
    #framework.run_comprehensive_tests()

    print("Testing complete. Check the generated visualizations and reports.")
